{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: Sean Deloddere <br>\n",
    "studentnr: 914691"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I opted not to solve Exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Identify most central and influential nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(nodes that have a higher value will be darker green, nodes with a lower value have a lighter colour.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### node degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Degree.PNG](Degree.png)\n",
    "![Degree_table.PNG](Degree_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of a node: the # of edges connected to that node. In our scenario an edge defines an interaction between people. This version is unweighed, so the length of the interaction doesn't matter. The nodes that have the highest degree aka the nodes with the most edges aka the people with the most interactions are seen in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighed degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![W_Degree.PNG](W_Degree.png)\n",
    "![W_Degree_table.PNG](W_Degree_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to degree, weighed degree takes into account the weight of the edge. The weighed degree of a node will be the sum of the weight of the edges of the node. In our scenario a long interaction will result in an edge with more weight. This means that the the nodes that score have the highest weight degree are the ones that correspond to the people that spent the most time interacting with others. Here it seems that 1437 and 1563 spent a lot of time interacting with each other, and this of course contributed to their total interaction duration. The 7 highest values are seen in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closeness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Closeness.PNG](Closeness.png)\n",
    "![Closeness_table.PNG](Closeness_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closeness centrality shows how efficiently information is able to be spread through the graph through the node. It measures its average farness, through shortest path, to all other nodes. In our scenario this would mean if Bob interacted with Stacy, and Stacy interacted with Lisa, and Lisa and Bob did not interact directly with each other, their farness would be 2. If you do this with all other people and take the average you would get the closeness centrality. The nodes with the highest closeness centrality are displayed in the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Betweenness centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Betweeness.PNG](betweeness.png)\n",
    "![Betweeness_table.PNG](betweeness_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betweenness centrality quantifies the number of times a node acts as a bridge along the shortest path between two other nodes. So, if a node is part of a lot of shortest paths between nodes, it will have a high betweenness score. In our scenario this would usually be a person that links 2 groups together for example.1443 seems to be the main link between nodes on the upper left side and nodes on the right, explaining its high score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not every measure ranks the same nodes as most central, one could have a lot of very short interactions for example and rank high in degree but low in weighed degree. However, it is fairly clear that in this case, most of top nodes for one measure are also relatively high for the others. 1477 is top 7 for every measure, and 1443 scores really high for all of them, barely not making top 7 for weighed degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Identify communities + c) Visualize communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modularity measures the degree to which parts of a system can be decoupled into seperate communities that interact more among themselves than with the other communities. This method is depending on a resolution parameter, a higher value results in less clusters, a lower value in more clusters. With resolution set to 1, I get the graph that can be seen below. It seems like the groups are somewhat corresponding to the classes and the genders, with a few exceptions. Blue is mostly class 5A and female, purple mostly class 5A and male, althouth with a fair amount of female as well. Orange is mostly 5B and female, and green is 5B and male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modularity.PNG](Modularity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting case is what we get when we set the resolution to 2, in this case it finds a clear distinction between the classes. All nodes in the purple community are from class A, all the ones from the blue community are in class B. This tells us that the class attribute is the biggest tell for which community the node will be in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modularity_2.PNG](Modularity_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Girvan-Newman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Girvan-Newman finds communities by progressively getting rid of more and more edges.The nodes that are still connected form communities. In the result below we can still somewhat see the 5A and 5B distinction, with their grey and purple colours. There are however a lot of additional 1-node communities, that are probably not that informative. 1430 also seems to be a weird outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![G_N.PNG](G_N.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Discuss communities and central nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modularity_d.PNG](modularity_d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We casn see that there are a lot of connections within the subclasses, especially the mostly female communty of class 5B is heavily interconnected, while in class 5A the 2 communties seem quite connected with each other. Looking at some central nodes, 1437 for example, the node that scored the heighest on the weighed degree, seems to be a link from orange (F-5B) to both  purple (mostly M-5A) and green (M-5B). Another interesting node is 1457, it seems like that is a girl from 5A that interacts quite a lot with other girls from 5B, but within her own class seems to interact more with the boys, hence her being in the more male dominated communty of 5A. Those different connections are also reflected in this node scoring high on the weighted degree list. 1443, a node with a high betweenness score, seems to quickly be the only link between blue(F-5A) and orange(F-5B) when increasing the treshold for the edges, so it is clear why its betweenness score is so high. Not all central nodes are bridge nodes though. An example of a central node that isn't a bridge node is node 1452. It scores 5th on the weighed degree list, but isn't connected strongly with any nodes outside of its community. However, the high score still makes sense since the node is so heavily connected within the community. One could conclude that some central nodes are bridges to other communities, while other central nodes are the main node within a community that connects to others within the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Propose a greedy algorithm to find a subgraph with good density according to this definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy algorithm I propose is to start off with the full graph and then iteratively drop the nodes with the lowest weighed degree and calculate the density of the remaining graph. The reason for dropping the node with the lowest weighed degree is that this will decrease the numerator the least out of dropping any other node, while the denominator will decrease the same regardless of what node is dropped. We can either continue dropping nodes untill no nodes are left, and then look at whichever subgraph had the highest density, or we can be extra greedy and stop straight away when we notice the density is no longer improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Evaluate the results on the dolphins.txt data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density(S,edges_dict):\n",
    "    nom = 0\n",
    "    if(len(edges_dict)==0):\n",
    "        return nom\n",
    "    for i in edges_dict.keys():\n",
    "        nom += len(edges_dict[i])\n",
    "    density = nom/S   \n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the density based on the formula provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_lowest(lowest_degree):\n",
    "    pruned=[]\n",
    "    for i in degrees.keys(): \n",
    "        if(degrees[i]==lowest_degree):\n",
    "            pruned.append(i)\n",
    "    for k in pruned:\n",
    "        for j in edges_dict[k]:\n",
    "            (edges_dict[j]).remove(k)\n",
    "            degrees[j]=degrees[j]-1\n",
    "             \n",
    "        del edges_dict[k]\n",
    "        del degrees[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prunes the node from the graph. It is deleted from the dictionaries that contain the degrees and the edges of all nodes, and its edges with other nodes are also removed from those nodes collections. Those nodes degrees are also lowered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum density: 5.660377358490566, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 27, 28, 29, 37, 42, 55, 3, 45, 62, 9, 4, 60, 52, 10, 6, 14, 57, 58, 7, 8, 31, 21, 38, 46, 33, 30, 34, 17, 25, 35, 39, 44, 51, 53, 19, 56, 26, 22, 24, 50, 40, 47, 54])\n",
      "maximum density: 5.956521739130435, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 27, 28, 29, 37, 42, 55, 3, 45, 62, 9, 4, 60, 52, 10, 6, 14, 58, 7, 8, 31, 21, 38, 46, 30, 34, 17, 25, 35, 39, 44, 51, 53, 19, 26, 22, 24])\n",
      "maximum density: 6.0, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 27, 28, 29, 37, 42, 55, 3, 45, 9, 4, 60, 52, 10, 6, 14, 58, 7, 8, 31, 21, 38, 46, 30, 34, 17, 25, 35, 39, 44, 51, 53, 19, 26, 22, 24])\n",
      "maximum density: 6.051282051282051, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 28, 29, 37, 42, 55, 45, 9, 60, 52, 10, 14, 58, 7, 8, 31, 21, 38, 46, 30, 34, 17, 25, 35, 39, 44, 51, 53, 19, 22])\n",
      "maximum density: 6.054054054054054, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 29, 37, 42, 55, 9, 60, 52, 10, 14, 58, 7, 8, 31, 21, 38, 46, 30, 34, 17, 25, 35, 39, 44, 51, 53, 19, 22])\n",
      "maximum density: 6.055555555555555, with subgraph:\n",
      "dict_keys([11, 1, 15, 16, 41, 43, 48, 18, 2, 20, 29, 37, 42, 55, 9, 60, 52, 10, 14, 58, 7, 8, 31, 21, 38, 46, 30, 34, 17, 25, 39, 44, 51, 53, 19, 22])\n"
     ]
    }
   ],
   "source": [
    "edges_temp = []\n",
    "degrees={}\n",
    "edges_dict={}\n",
    "edges = []\n",
    "file = open(\"dolphins.txt\",\"r\")\n",
    "lines = file.readlines()\n",
    "num_nodes = int(lines[0])\n",
    "edges_temp = lines[1:]\n",
    "for i in edges_temp:\n",
    "    edge = list(map(int,i.split()))\n",
    "    edges.append(edge)\n",
    "    if edge[0] not in edges_dict.keys():\n",
    "        degrees[edge[0]]=0\n",
    "        edges_dict[edge[0]]=[]\n",
    "    degrees[edge[0]]+=1\n",
    "    edges_dict[edge[0]].append(edge[1])\n",
    "\n",
    "    if edge[1] not in edges_dict.keys():\n",
    "        degrees[edge[1]]=0\n",
    "        edges_dict[edge[1]]=[]\n",
    "    degrees[edge[1]]+=1\n",
    "    edges_dict[edge[1]].append(edge[0])\n",
    "file.close()\n",
    "#print(edges_dict)\n",
    "#print(degrees)\n",
    "\n",
    "\n",
    "maximum_density = 0\n",
    "subgraph = []\n",
    "while(len(degrees)>0):\n",
    "    lowest_degree = 50000000000\n",
    "    \n",
    "    #look for node with lowest degree and prune it\n",
    "    for i in degrees.keys():\n",
    "        lowest_degree = min(degrees[i],lowest_degree)\n",
    "    prune_lowest(lowest_degree)\n",
    "    \n",
    "     #check if density after pruning went up, if so, store new density in place of old one as the max density.\n",
    "    if(calculate_density(len(degrees),edges_dict)>maximum_density):\n",
    "        maximum_density = calculate_density(len(degrees),edges_dict)\n",
    "        print(\"maximum density: \"+str(maximum_density)+\", with subgraph:\")\n",
    "        subgraph = edges_dict.keys()\n",
    "        print(subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically done in code what I explained in words in a). The resulting subgraph and its density are the last line printed above. I couldn't figure out how to only print the last line, since it didn't want to print outside of the for loops, excuse me for the inconvenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I opted not to solve exercise 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_5</th>\n",
       "      <th>joke_7</th>\n",
       "      <th>joke_8</th>\n",
       "      <th>joke_13</th>\n",
       "      <th>joke_15</th>\n",
       "      <th>joke_16</th>\n",
       "      <th>joke_17</th>\n",
       "      <th>joke_18</th>\n",
       "      <th>joke_19</th>\n",
       "      <th>joke_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_5013</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_10016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_21844</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_3403</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_23240</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            joke_5  joke_7  joke_8  joke_13  joke_15  joke_16  joke_17  \\\n",
       "user_id                                                                  \n",
       "user_5013        0       1       0        0        1        0        0   \n",
       "user_10016       0       0       0        1        0        1        1   \n",
       "user_21844       1       1       1        1        1        1        1   \n",
       "user_3403        1       0       0        1        0        1        1   \n",
       "user_23240       0       1       0        1        1        1        1   \n",
       "\n",
       "            joke_18  joke_19  joke_20  \n",
       "user_id                                \n",
       "user_5013         1        1        1  \n",
       "user_10016        1        0        1  \n",
       "user_21844        1        1        1  \n",
       "user_3403         0        1        1  \n",
       "user_23240        1        0        1  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('jester-800-10.csv', index_col = 0, usecols=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "test_data = pd.read_csv('jester-800-10.csv', index_col = 0, usecols=[1,2,3,4,5,6,7,8,9,10,11])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke_5</th>\n",
       "      <th>joke_7</th>\n",
       "      <th>joke_8</th>\n",
       "      <th>joke_13</th>\n",
       "      <th>joke_15</th>\n",
       "      <th>joke_16</th>\n",
       "      <th>joke_17</th>\n",
       "      <th>joke_18</th>\n",
       "      <th>joke_19</th>\n",
       "      <th>joke_20</th>\n",
       "      <th>...</th>\n",
       "      <th>user_4442</th>\n",
       "      <th>user_15882</th>\n",
       "      <th>user_10237</th>\n",
       "      <th>user_16014</th>\n",
       "      <th>user_12180</th>\n",
       "      <th>user_7471</th>\n",
       "      <th>user_22586</th>\n",
       "      <th>user_14652</th>\n",
       "      <th>user_15506</th>\n",
       "      <th>user_5338</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_5013</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_10016</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_21844</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_3403</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_23240</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 810 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            joke_5  joke_7  joke_8  joke_13  joke_15  joke_16  joke_17  \\\n",
       "user_5013      0.0     1.0     0.0      0.0      1.0      0.0      0.0   \n",
       "user_10016     0.0     0.0     0.0      1.0      0.0      1.0      1.0   \n",
       "user_21844     1.0     1.0     1.0      1.0      1.0      1.0      1.0   \n",
       "user_3403      1.0     0.0     0.0      1.0      0.0      1.0      1.0   \n",
       "user_23240     0.0     1.0     0.0      1.0      1.0      1.0      1.0   \n",
       "\n",
       "            joke_18  joke_19  joke_20  ...  user_4442  user_15882  user_10237  \\\n",
       "user_5013       1.0      1.0      1.0  ...        0.0         0.0         0.0   \n",
       "user_10016      1.0      0.0      1.0  ...        0.0         0.0         0.0   \n",
       "user_21844      1.0      1.0      1.0  ...        0.0         0.0         0.0   \n",
       "user_3403       0.0      1.0      1.0  ...        0.0         0.0         0.0   \n",
       "user_23240      1.0      0.0      1.0  ...        0.0         0.0         0.0   \n",
       "\n",
       "            user_16014  user_12180  user_7471  user_22586  user_14652  \\\n",
       "user_5013          0.0         0.0        0.0         0.0         0.0   \n",
       "user_10016         0.0         0.0        0.0         0.0         0.0   \n",
       "user_21844         0.0         0.0        0.0         0.0         0.0   \n",
       "user_3403          0.0         0.0        0.0         0.0         0.0   \n",
       "user_23240         0.0         0.0        0.0         0.0         0.0   \n",
       "\n",
       "            user_15506  user_5338  \n",
       "user_5013          0.0        0.0  \n",
       "user_10016         0.0        0.0  \n",
       "user_21844         0.0        0.0  \n",
       "user_3403          0.0        0.0  \n",
       "user_23240         0.0        0.0  \n",
       "\n",
       "[5 rows x 810 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_t = train_data.T\n",
    "Transition_df = pd.concat([train_data,train_data_t],axis=0).fillna(0)\n",
    "connection_matrix = Transition_df.to_numpy()\n",
    "Transition_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I transpose the matrix and add it to the original matrix so that I not only have transitions from users to jokes, but also from jokes to users. Direct transitions between users and between jokes are obviously set to 0, since they are impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K most similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours_list = []\n",
    "for i in range(len(connection_matrix)):\n",
    "    neighbours = []\n",
    "    for j in range(len(connection_matrix[i])):\n",
    "        if connection_matrix[i][j]>0:\n",
    "            neighbours.append(j)\n",
    "    neighbours_list.append(neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I precomputed all the neighbouring nodes for each node to save runtime during the SimRank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimRank(i,j,k):\n",
    "    if(i==j):\n",
    "        return 1\n",
    "    else:    \n",
    "        if(len(neighbours_list[i]) == 0 or len(neighbours_list[j]) == 0 or k >= 2):\n",
    "            return 0\n",
    "        else:\n",
    "            x = C/(len(neighbours_list[i])*len(neighbours_list[j]))\n",
    "            sr = 0\n",
    "            for p in neighbours_list[i]:\n",
    "                for q in neighbours_list[j]:\n",
    "                    sr += x * SimRank(p,q,k+1)\n",
    "            return sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very dense graph with a lot of edges (jokes have hundreds of edges), without a limit it can take very long to run. So, in order to lower computation time, I added a k to the algorithm. This k will make it so that when the algorithm goes too deep recursively, it will return 0. This shouldn't be too harmful to the outcome, because if it takes too many steps to get to another node, it isn't a very close node anyway (at least in this particular graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9% done\n",
      "9.9% done\n",
      "14.8% done\n",
      "19.8% done\n",
      "24.7% done\n",
      "29.6% done\n",
      "34.6% done\n",
      "39.5% done\n",
      "44.4% done\n",
      "49.4% done\n",
      "54.3% done\n",
      "59.3% done\n",
      "64.2% done\n",
      "69.1% done\n",
      "74.1% done\n",
      "79.0% done\n",
      "84.0% done\n",
      "88.9% done\n",
      "93.8% done\n",
      "98.8% done\n"
     ]
    }
   ],
   "source": [
    "C = 0.8\n",
    "SimRanks = np.zeros((800,800))\n",
    "for i in range(len(connection_matrix)-10):\n",
    "    for j in range(len(connection_matrix)-10):\n",
    "        SimRanks[i][j] = SimRank(i,j,0)\n",
    "    if(i%40 == 39):\n",
    "        print(str(np.round(((i+1)/8.1),1))+\"% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimRanks now contains the similarity of each user to every other user. Next we sort every row based on this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 358 545 ... 242  52 239]\n",
      " [  1 110 444 ... 230 727 629]\n",
      " [  2 545 358 ... 630  52 239]\n",
      " ...\n",
      " [797 545 358 ... 540 162 554]\n",
      " [798 255 159 ... 474 691 638]\n",
      " [799 575 691 ... 434 666 261]]\n"
     ]
    }
   ],
   "source": [
    "SimRanks_sorted = np.argsort(SimRanks, axis = 1)[:,::-1]\n",
    "print(SimRanks_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column (column 0) is always the user itself, which makes sense, every user is the most similar to itself. So when we want the K (=20) most similar users, we take column 1 to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[358 545 537 ... 198 312 325]\n",
      " [110 444  14 ... 520 590  92]\n",
      " [545 358   0 ... 161 234 307]\n",
      " ...\n",
      " [545 358 548 ...  55 204 205]\n",
      " [255 159 734 ... 548 482 345]\n",
      " [575 691  27 ... 226 103  24]]\n"
     ]
    }
   ],
   "source": [
    "k_most_sim = SimRanks_sorted[:,1:20]\n",
    "print(k_most_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the 20 most similar users for each user, so we have completed the first phase. Let's check a few examples to see whether the users are indeed similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 1 1 1]\n",
      "[0 1 0 0 1 0 0 1 0 1]\n",
      "[0 1 0 0 1 0 0 1 0 1]\n",
      "[0 1 0 0 1 0 0 1 1 1]\n",
      "[0 1 0 1 1 0 0 1 1 1]\n",
      "[0 1 0 1 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "joke_scores = train_data.to_numpy()\n",
    "print(joke_scores[0])\n",
    "for i in k_most_sim[0][0:5]:\n",
    "    print(joke_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the most similar users differ from each other by at most 1 joke, so looks like the algorithm worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 20 most similar users for each user, we can look within this group to find the best joke to recommend to each user. The way I will approach this is going over every user in the similar user list and looking at the scores they gave to jokes. For every user that gave a positive review to a joke, I increase that jokes score by 1. At the end, I have a score of how the users as a group liked every joke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cummulative_joke_score(most_sim):\n",
    "    cumul_joke_scores = [0,0,0,0,0,0,0,0,0,0]\n",
    "    for user in most_sim:\n",
    "        for i in range(len(joke_scores[user])):\n",
    "            cumul_joke_scores[i] += joke_scores[user][i]\n",
    "    return cumul_joke_scores        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up I can use this score to recommend a joke to every user. I use argsort to get the jokes that received the highest scores from the group, and then I go from best joke to worst joke untill I find one that the user has not reviewed yet. As soon as I find one I stop and recommend that joke. If the user has already reviewed all jokes, I just return that I can't recommend anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_joke(own_joke_scores, cumul_joke_scores):\n",
    "    joke_scores_sorted = np.argsort(cumul_joke_scores, axis = 0)[::-1]\n",
    "    for i in joke_scores_sorted:\n",
    "        if(own_joke_scores[i] == 0):\n",
    "            return train_data.columns[i]\n",
    "    return \"nothing, you already reviewed all jokes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demonstration of the recommendations for all the users in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "for i in range(800):\n",
    "    c_joke_score = cummulative_joke_score(k_most_sim[i])\n",
    "    recommendation = recommend_joke(joke_scores[i],c_joke_score)\n",
    "    recommendations.append(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For user: user_5013, I recommend joke_13.\n",
      "For user: user_10016, I recommend joke_7.\n",
      "For user: user_21844, I recommend nothing, you already reviewed all jokes.\n",
      "For user: user_3403, I recommend joke_18.\n",
      "For user: user_23240, I recommend joke_19.\n",
      "For user: user_835, I recommend nothing, you already reviewed all jokes.\n",
      "For user: user_1538, I recommend joke_18.\n",
      "For user: user_7229, I recommend joke_17.\n",
      "For user: user_10809, I recommend joke_19.\n",
      "For user: user_14356, I recommend joke_17.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"For user: \"+str(train_data.index[i])+\", I recommend \"+str(recommendations[i])+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to now recommend a joke to a completely new person, e.g. from the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_matrix = test_data.to_numpy()\n",
    "new_user_scores = test_matrix[0]\n",
    "print(new_user_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can first check whether I have the same scores already in the training set, and if so, just give the same recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this new user, I recommend joke_13\n"
     ]
    }
   ],
   "source": [
    "train_matrix = train_data.to_numpy()\n",
    "for i in range(len(train_matrix)):\n",
    "    if new_user_scores.all() == train_matrix[i].all():\n",
    "        c_joke_score = cummulative_joke_score(k_most_sim[i])\n",
    "        recommendation = recommend_joke(joke_scores[i],c_joke_score)\n",
    "        break\n",
    "print(\"For this new user, I recommend \"+str(recommendation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new user is actually identical to the first user in the train data, so it makes sense that they get the same suggestion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the test person is not yet in the training set, I can go over the steps again of finding its most similar users, summing up their scores and recommending the best joke that the user has not reviewed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Modify code so that features are based on frequency of words. Compare accuracy with and without preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct list of documents labeled with the appropriate categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Throughout the notebook I will refer to the classifiers using these features as \"contain\" classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features_contained(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cast document to a set because it is much faster to look if an element occurs in a set than in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Throughout the notebook I will refer to the classifiers using these features as \"frequency\" classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features_frequencies(document):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = 0\n",
    "    for word_ in document:\n",
    "        if word_ in word_features:\n",
    "            features[word_] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to look at how frequently a word occurs in the document, we can not cast it to a set, because then dupplicates are lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier that looks only at whether a word occurs or not in a document: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features_contained(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_c = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of classifier that looks only at whether a word occurs or not in a document: \"+str(nltk.classify.accuracy(classifier_c, test_set))+\"\\n\") \n",
    "#classifier_c.show_most_informative_features(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier looks at how frequently a word occurs in a document: 0.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features_frequencies(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_f = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of classifier looks at how frequently a word occurs in a document: \"+str(nltk.classify.accuracy(classifier_f, test_set))+\"\\n\") \n",
    "#classifier_f.show_most_informative_features(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: These accuracies are not always the same due to the shuffling of the documents so that different documents are in the training and testing set. I am discussing results I saw most frequently.\n",
    "\n",
    "It seems that without doing any preprocessing, taking into account the frequency (amount of occurences) of words in the documents, as the frequency classifier does, does not improve the accuracy, in fact, most of the time it makes it worse. One of the reasons for this decrease in accuracy could be that the frequency classifier takes words into account that occur a lot, but aren't really that informative. More about that in the motivation of why to remove stopwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivation: When we look at the 35 most informative features, we see that the ones for the contains classifier make a lot of sense, while in the ones for frequency classifier, there are words like 'who', 'and', 'is', 'of', 'to', that, to a human, don't really look they are informative on wether or not a review will be possible or not. These are all common stopwords, so I hypothesize that the frequency classifier performs better after removing these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the documents are already tokenized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capsule', ':', 'the', 'world', 'will', 'come', 'to', 'an', 'end', 'at', 'midnight', '.', 'everyone', 'knows', 'it', 'and', 'must', 'make', 'a', 'final']\n"
     ]
    }
   ],
   "source": [
    "print(documents[1][0][0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can move on straight to stop word removal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with stopwords: \n",
      "['starring', 'ben', 'stiller', ',', 'elizabeth', 'hurley', ',', 'maria', 'bello', ',', 'janeane', 'garofalo', 'screenplay', 'by', 'david', 'veloz', ',', 'based', 'on', 'the']\n",
      "\n",
      "without stopwords: \n",
      "['starring', 'ben', 'stiller', ',', 'elizabeth', 'hurley', ',', 'maria', 'bello', ',', 'janeane', 'garofalo', 'screenplay', 'david', 'veloz', ',', 'based', 'novel', 'jerry', 'stahl']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_documents = []\n",
    "for document in documents:\n",
    "    filtered_document = [token for token in document[0] if token not in stop_words]\n",
    "    filtered_documents.append([filtered_document, document[1]])\n",
    "\n",
    "print(\"with stopwords: \")\n",
    "print(documents[0][0][:20])\n",
    "print(\"\\nwithout stopwords: \")\n",
    "print(filtered_documents[0][0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier that looks only at whether a word occurs or not in a document: 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features_contained(d), c) for (d,c) in filtered_documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_c = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of classifier that looks only at whether a word occurs or not in a document: \"+str(nltk.classify.accuracy(classifier_c, test_set))+\"\\n\") \n",
    "#classifier_c.show_most_informative_features(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classifier looks at how frequently a word occurs in a document: 0.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features_frequencies(d), c) for (d,c) in filtered_documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_f = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of classifier looks at how frequently a word occurs in a document: \"+str(nltk.classify.accuracy(classifier_f, test_set))+\"\\n\") \n",
    "#classifier_f.show_most_informative_features(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the accuracy for the contains classifier sometimes decreases, the accuracy for the frequency classifier almost always improves, as I hypothesized. Another thing to notice is that 4 is now a word in the most informative words of the frequency classifier, so it is likely that removing numbers could have a positive effect on the accuracy as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Augment the movie review document to use hypernyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing words in documents by hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernyms_documents = []\n",
    "for (d,c) in filtered_documents:\n",
    "    hypernyms_document = []\n",
    "    for word in d:\n",
    "        synset = wn.synsets(word)\n",
    "        #if not empty:\n",
    "        if(synset):\n",
    "            hypernym = synset[0].hypernyms()\n",
    "            #if not empty:\n",
    "            if(hypernym):\n",
    "                lemma = hypernym[0].lemma_names()\n",
    "                #if not empty\n",
    "                if(lemma):\n",
    "                    lemma_l = lemma[0].lower()\n",
    "                    hypernyms_document.append(lemma_l)\n",
    "    hypernyms_documents.append((hypernyms_document, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing most frequent words by hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features_only_hypernyms = []\n",
    "for word in word_features:\n",
    "    synset = wn.synsets(word)\n",
    "    #if not empty:\n",
    "    if(synset):\n",
    "        hypernym = synset[0].hypernyms()\n",
    "        #if not empty:\n",
    "        if(hypernym):\n",
    "            lemma = hypernym[0].lemma_names()\n",
    "            #if not empty:\n",
    "            if(lemma):\n",
    "                lemma_l = lemma[0].lower()\n",
    "                word_features_only_hypernyms.append(lemma_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assumption made: the first synset, hypernym and lemma are the most representative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made this assumption mainly to save computing time, because it take incredible long to run while incorporating all the synsets and hypernyms. A more nuanced implementation could of course result in a higher eventual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slightly adapting feature functions to take into account the different frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features_contained_2(document,word_f):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_f:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features_frequencies_2(document,word_f):\n",
    "    features = {}\n",
    "    for word in word_f:\n",
    "        features[word] = 0\n",
    "    for word_ in document:\n",
    "        if word_ in word_f:\n",
    "            features[word_] += 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train + test contain classifier using only hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5% done.\n",
      "1.0% done.\n",
      "1.5% done.\n",
      "2.0% done.\n",
      "2.5% done.\n",
      "3.0% done.\n",
      "3.5% done.\n",
      "4.0% done.\n",
      "4.5% done.\n",
      "5.0% done.\n",
      "5.5% done.\n",
      "6.0% done.\n",
      "6.5% done.\n",
      "7.0% done.\n",
      "7.5% done.\n",
      "8.0% done.\n",
      "8.5% done.\n",
      "9.0% done.\n",
      "9.5% done.\n",
      "10.0% done.\n",
      "10.5% done.\n",
      "11.0% done.\n",
      "11.5% done.\n",
      "12.0% done.\n",
      "12.5% done.\n",
      "13.0% done.\n",
      "13.5% done.\n",
      "14.0% done.\n",
      "14.5% done.\n",
      "15.0% done.\n",
      "15.5% done.\n",
      "16.0% done.\n",
      "16.5% done.\n",
      "17.0% done.\n",
      "17.5% done.\n",
      "18.0% done.\n",
      "18.5% done.\n",
      "19.0% done.\n",
      "19.5% done.\n",
      "20.0% done.\n",
      "20.5% done.\n",
      "21.0% done.\n",
      "21.5% done.\n",
      "22.0% done.\n",
      "22.5% done.\n",
      "23.0% done.\n",
      "23.5% done.\n",
      "24.0% done.\n",
      "24.5% done.\n",
      "25.0% done.\n",
      "25.5% done.\n",
      "26.0% done.\n",
      "26.5% done.\n",
      "27.0% done.\n",
      "27.5% done.\n",
      "28.0% done.\n",
      "28.5% done.\n",
      "29.0% done.\n",
      "29.5% done.\n",
      "30.0% done.\n",
      "30.5% done.\n",
      "31.0% done.\n",
      "31.5% done.\n",
      "32.0% done.\n",
      "32.5% done.\n",
      "33.0% done.\n",
      "33.5% done.\n",
      "34.0% done.\n",
      "34.5% done.\n",
      "35.0% done.\n",
      "35.5% done.\n",
      "36.0% done.\n",
      "36.5% done.\n",
      "37.0% done.\n",
      "37.5% done.\n",
      "38.0% done.\n",
      "38.5% done.\n",
      "39.0% done.\n",
      "39.5% done.\n",
      "40.0% done.\n",
      "40.5% done.\n",
      "41.0% done.\n",
      "41.5% done.\n",
      "42.0% done.\n",
      "42.5% done.\n",
      "43.0% done.\n",
      "43.5% done.\n",
      "44.0% done.\n",
      "44.5% done.\n",
      "45.0% done.\n",
      "45.5% done.\n",
      "46.0% done.\n",
      "46.5% done.\n",
      "47.0% done.\n",
      "47.5% done.\n",
      "48.0% done.\n",
      "48.5% done.\n",
      "49.0% done.\n",
      "49.5% done.\n",
      "50.0% done.\n",
      "50.5% done.\n",
      "51.0% done.\n",
      "51.5% done.\n",
      "52.0% done.\n",
      "52.5% done.\n",
      "53.0% done.\n",
      "53.5% done.\n",
      "54.0% done.\n",
      "54.5% done.\n",
      "55.0% done.\n",
      "55.5% done.\n",
      "56.0% done.\n",
      "56.5% done.\n",
      "57.0% done.\n",
      "57.5% done.\n",
      "58.0% done.\n",
      "58.5% done.\n",
      "59.0% done.\n",
      "59.5% done.\n",
      "60.0% done.\n",
      "60.5% done.\n",
      "61.0% done.\n",
      "61.5% done.\n",
      "62.0% done.\n",
      "62.5% done.\n",
      "63.0% done.\n",
      "63.5% done.\n",
      "64.0% done.\n",
      "64.5% done.\n",
      "65.0% done.\n",
      "65.5% done.\n",
      "66.0% done.\n",
      "66.5% done.\n",
      "67.0% done.\n",
      "67.5% done.\n",
      "68.0% done.\n",
      "68.5% done.\n",
      "69.0% done.\n",
      "69.5% done.\n",
      "70.0% done.\n",
      "70.5% done.\n",
      "71.0% done.\n",
      "71.5% done.\n",
      "72.0% done.\n",
      "72.5% done.\n",
      "73.0% done.\n",
      "73.5% done.\n",
      "74.0% done.\n",
      "74.5% done.\n",
      "75.0% done.\n",
      "75.5% done.\n",
      "76.0% done.\n",
      "76.5% done.\n",
      "77.0% done.\n",
      "77.5% done.\n",
      "78.0% done.\n",
      "78.5% done.\n",
      "79.0% done.\n",
      "79.5% done.\n",
      "80.0% done.\n",
      "80.5% done.\n",
      "81.0% done.\n",
      "81.5% done.\n",
      "82.0% done.\n",
      "82.5% done.\n",
      "83.0% done.\n",
      "83.5% done.\n",
      "84.0% done.\n",
      "84.5% done.\n",
      "85.0% done.\n",
      "85.5% done.\n",
      "86.0% done.\n",
      "86.5% done.\n",
      "87.0% done.\n",
      "87.5% done.\n",
      "88.0% done.\n",
      "88.5% done.\n",
      "89.0% done.\n",
      "89.5% done.\n",
      "90.0% done.\n",
      "90.5% done.\n",
      "91.0% done.\n",
      "91.5% done.\n",
      "92.0% done.\n",
      "92.5% done.\n",
      "93.0% done.\n",
      "93.5% done.\n",
      "94.0% done.\n",
      "94.5% done.\n",
      "95.0% done.\n",
      "95.5% done.\n",
      "96.0% done.\n",
      "96.5% done.\n",
      "97.0% done.\n",
      "97.5% done.\n",
      "98.0% done.\n",
      "98.5% done.\n",
      "99.0% done.\n",
      "99.5% done.\n",
      "100.0% done.\n"
     ]
    }
   ],
   "source": [
    "featuresets = []\n",
    "i=0\n",
    "for (d,c) in hypernyms_documents:\n",
    "    featuresets.append((document_features_contained_2(d,word_features_only_hypernyms), c))\n",
    "    if(i%10 == 9):\n",
    "        print(str((i+1)/20)+\"% done.\")\n",
    "    i += 1\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_c_oh = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of contains classifier with only hypernyms: 0.66\n",
      "\n",
      "Most Informative Features\n",
      "  contains(word_picture) = True              pos : neg    =      3.5 : 1.0\n",
      "contains(disorderliness) = True              neg : pos    =      3.3 : 1.0\n",
      "     contains(simpleton) = True              neg : pos    =      3.1 : 1.0\n",
      "     contains(gentleman) = True              neg : pos    =      2.9 : 1.0\n",
      "       contains(diapsid) = True              neg : pos    =      2.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of contains classifier with only hypernyms: \"+str(nltk.classify.accuracy(classifier_c_oh, test_set))+\"\\n\") \n",
    "classifier_c_oh.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train + test frequency classifier using only hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5% done.\n",
      "1.0% done.\n",
      "1.5% done.\n",
      "2.0% done.\n",
      "2.5% done.\n",
      "3.0% done.\n",
      "3.5% done.\n",
      "4.0% done.\n",
      "4.5% done.\n",
      "5.0% done.\n",
      "5.5% done.\n",
      "6.0% done.\n",
      "6.5% done.\n",
      "7.0% done.\n",
      "7.5% done.\n",
      "8.0% done.\n",
      "8.5% done.\n",
      "9.0% done.\n",
      "9.5% done.\n",
      "10.0% done.\n",
      "10.5% done.\n",
      "11.0% done.\n",
      "11.5% done.\n",
      "12.0% done.\n",
      "12.5% done.\n",
      "13.0% done.\n",
      "13.5% done.\n",
      "14.0% done.\n",
      "14.5% done.\n",
      "15.0% done.\n",
      "15.5% done.\n",
      "16.0% done.\n",
      "16.5% done.\n",
      "17.0% done.\n",
      "17.5% done.\n",
      "18.0% done.\n",
      "18.5% done.\n",
      "19.0% done.\n",
      "19.5% done.\n",
      "20.0% done.\n",
      "20.5% done.\n",
      "21.0% done.\n",
      "21.5% done.\n",
      "22.0% done.\n",
      "22.5% done.\n",
      "23.0% done.\n",
      "23.5% done.\n",
      "24.0% done.\n",
      "24.5% done.\n",
      "25.0% done.\n",
      "25.5% done.\n",
      "26.0% done.\n",
      "26.5% done.\n",
      "27.0% done.\n",
      "27.5% done.\n",
      "28.0% done.\n",
      "28.5% done.\n",
      "29.0% done.\n",
      "29.5% done.\n",
      "30.0% done.\n",
      "30.5% done.\n",
      "31.0% done.\n",
      "31.5% done.\n",
      "32.0% done.\n",
      "32.5% done.\n",
      "33.0% done.\n",
      "33.5% done.\n",
      "34.0% done.\n",
      "34.5% done.\n",
      "35.0% done.\n",
      "35.5% done.\n",
      "36.0% done.\n",
      "36.5% done.\n",
      "37.0% done.\n",
      "37.5% done.\n",
      "38.0% done.\n",
      "38.5% done.\n",
      "39.0% done.\n",
      "39.5% done.\n",
      "40.0% done.\n",
      "40.5% done.\n",
      "41.0% done.\n",
      "41.5% done.\n",
      "42.0% done.\n",
      "42.5% done.\n",
      "43.0% done.\n",
      "43.5% done.\n",
      "44.0% done.\n",
      "44.5% done.\n",
      "45.0% done.\n",
      "45.5% done.\n",
      "46.0% done.\n",
      "46.5% done.\n",
      "47.0% done.\n",
      "47.5% done.\n",
      "48.0% done.\n",
      "48.5% done.\n",
      "49.0% done.\n",
      "49.5% done.\n",
      "50.0% done.\n",
      "50.5% done.\n",
      "51.0% done.\n",
      "51.5% done.\n",
      "52.0% done.\n",
      "52.5% done.\n",
      "53.0% done.\n",
      "53.5% done.\n",
      "54.0% done.\n",
      "54.5% done.\n",
      "55.0% done.\n",
      "55.5% done.\n",
      "56.0% done.\n",
      "56.5% done.\n",
      "57.0% done.\n",
      "57.5% done.\n",
      "58.0% done.\n",
      "58.5% done.\n",
      "59.0% done.\n",
      "59.5% done.\n",
      "60.0% done.\n",
      "60.5% done.\n",
      "61.0% done.\n",
      "61.5% done.\n",
      "62.0% done.\n",
      "62.5% done.\n",
      "63.0% done.\n",
      "63.5% done.\n",
      "64.0% done.\n",
      "64.5% done.\n",
      "65.0% done.\n",
      "65.5% done.\n",
      "66.0% done.\n",
      "66.5% done.\n",
      "67.0% done.\n",
      "67.5% done.\n",
      "68.0% done.\n",
      "68.5% done.\n",
      "69.0% done.\n",
      "69.5% done.\n",
      "70.0% done.\n",
      "70.5% done.\n",
      "71.0% done.\n",
      "71.5% done.\n",
      "72.0% done.\n",
      "72.5% done.\n",
      "73.0% done.\n",
      "73.5% done.\n",
      "74.0% done.\n",
      "74.5% done.\n",
      "75.0% done.\n",
      "75.5% done.\n",
      "76.0% done.\n",
      "76.5% done.\n",
      "77.0% done.\n",
      "77.5% done.\n",
      "78.0% done.\n",
      "78.5% done.\n",
      "79.0% done.\n",
      "79.5% done.\n",
      "80.0% done.\n",
      "80.5% done.\n",
      "81.0% done.\n",
      "81.5% done.\n",
      "82.0% done.\n",
      "82.5% done.\n",
      "83.0% done.\n",
      "83.5% done.\n",
      "84.0% done.\n",
      "84.5% done.\n",
      "85.0% done.\n",
      "85.5% done.\n",
      "86.0% done.\n",
      "86.5% done.\n",
      "87.0% done.\n",
      "87.5% done.\n",
      "88.0% done.\n",
      "88.5% done.\n",
      "89.0% done.\n",
      "89.5% done.\n",
      "90.0% done.\n",
      "90.5% done.\n",
      "91.0% done.\n",
      "91.5% done.\n",
      "92.0% done.\n",
      "92.5% done.\n",
      "93.0% done.\n",
      "93.5% done.\n",
      "94.0% done.\n",
      "94.5% done.\n",
      "95.0% done.\n",
      "95.5% done.\n",
      "96.0% done.\n",
      "96.5% done.\n",
      "97.0% done.\n",
      "97.5% done.\n",
      "98.0% done.\n",
      "98.5% done.\n",
      "99.0% done.\n",
      "99.5% done.\n",
      "100.0% done.\n"
     ]
    }
   ],
   "source": [
    "featuresets = []\n",
    "i=0\n",
    "for (d,c) in hypernyms_documents:\n",
    "    featuresets.append((document_features_frequencies_2(d,word_features_only_hypernyms), c))\n",
    "    if(i%10 == 9):\n",
    "        print(str((i+1)/20)+\"% done.\")\n",
    "    i += 1\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier_f_oh = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of frequency classifier with only hypernyms: 0.65\n",
      "\n",
      "Most Informative Features\n",
      "                   being = 7                 pos : neg    =      9.3 : 1.0\n",
      "               simpleton = 3                 neg : pos    =      9.1 : 1.0\n",
      "                    kind = 12                neg : pos    =      8.1 : 1.0\n",
      "                 society = 2                 pos : neg    =      7.9 : 1.0\n",
      "             orientation = 3                 pos : neg    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of frequency classifier with only hypernyms: \"+str(nltk.classify.accuracy(classifier_f_oh, test_set))+\"\\n\") \n",
    "classifier_f_oh.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that both the classifiers perform worse when using the hypernyms over the actual words. The frequency classifier seems to even have 'kind = 12' as a feature that indicates negative sentiment, which seems very strange. However, this could also be because a hyponym (the inverse of hypernym) of 'kind' is used frequently with a 'not' before it, but since 'not' is a stopword and we drop it, this meaning gets lost. This doesn't mean that the use of hypernyms is bad, this was a pretty brute force, not nuanced way of implementing hypernyms. Replacing words with their hypernym might make them more general, but it also loses parts of their meaning. I highly suspect that replacing only the less frequent words, that actually have something to gain from being more general, by their hypernyms, would result in a better accuracy. On top of that, I also made the assumption mentioned above. Including all synsets and hypernyms could also hurt the accuracy, but perhaps there is a better way of choosing which ones to use that could improve the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
